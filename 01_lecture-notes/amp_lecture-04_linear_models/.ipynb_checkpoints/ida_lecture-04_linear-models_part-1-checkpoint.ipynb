{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Лекція 4. Лінійні моделі класифікації і регресії</center>\n",
    "## <center>Частина 1</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зміст \n",
    "\n",
    "- [4.1. Лінійна регресія](#4.1)\n",
    "    + [4.1.1. Метод найменших квадратів](#4.1.1)\n",
    "    + [4.1.2. Метод максимальної правдоподібності](#4.1.2)\n",
    "    + [4.1.3. Розкладання помилки на зміщення і розкид (Bias-variance decomposition)](#4.1.3)\n",
    "    + [4.1.4. Регуляризація лінійної регресії](#4.1.4)\n",
    "    \n",
    "- [4.2. Логістична регресія та метод максимальної правдоподібності](#4.2)\n",
    "    + [4.2.1. Лінійний класифікатор](#4.2.1)\n",
    "    + [4.2.2. Логістична регресія, як лінійний класифікатор](#4.2.2)\n",
    "    + [4.2.3. Принцип максимальної правдоподібності та логістична регресія](#4.2.3)\n",
    "    + [4.2.4. L2-регуляризація логістичної функції втрат](#4.2.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">4.1. Лінійна регресія</span>\n",
    "\n",
    "[Повернутися до змісту](#4-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.1.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue; font-size:1em;\">4.1.1 Метод найменших квадратів</span>\n",
    "\n",
    "[Повернутися до змісту](#4-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Розповідь про лінійні моделі ми почнемо з лінійної регресії. В першу чергу, необхідно задати модель залежності цільової змінної $y$ від чинників, що її пояснюють; функція залежності буде лінійною: $y = w_0 + \\sum_{i=1}^m w_i x_i$. Якщо ми додамо фіктивну розмірність $x_0 = 1$ для кожного спостереження, тоді лінійну форму можна переписати більш компактно, записавши вільний член $w_0$ під суму: $y = \\sum_{i=0}^m w_i x_i = \\textbf{w}^{\\text{T}} \\textbf{x}$. Якщо розглядати матрицю спостереження-ознаки, у якій в рядках знаходяться приклади з набору даних, то нам необхідно додати одиничну колонку зліва. Задамо модель наступним чином:\n",
    "\n",
    "$$\\large \\textbf y = \\textbf{X} \\textbf w + \\epsilon,$$\n",
    "\n",
    "де\n",
    "- $\\textbf{y} \\in \\mathbb{R}^n$ – цільова змінна;\n",
    "- $\\textbf{w}\\in \\mathbb{R}^{m+1}$ – вектор параметрів моделі (в машинному навчанні (МН) ці параметри часто називають вагами);\n",
    "- $\\textbf{X}$ – матриця спостережень й ознак розмірності $n$ рядків на $m + 1$ стовпців (включаючи фіктивну одиничну колонку зліва) з повним рангом за стовпцями: $\\text{rank}\\left(\\textbf{X}\\right) = m$;\n",
    "- $\\epsilon$ – випадкова змінна, що відповідає випадковій, непрогнозованій помилці моделі.\n",
    "\n",
    "Можемо виписати вираз для кожного конкретного спостереження\n",
    "\n",
    "$$\\large \n",
    "y_i = \\sum_{j=0}^m w_j X_{ij} + \\epsilon_i$$\n",
    "\n",
    "Також на модель накладаються такі обмеження (інакше це буде якась інша регресія, але точно не лінійна):\n",
    "- математичне очікування випадкових помилок дорівнює нулю: $\\forall i: \\mathbb{E}\\left[\\epsilon_i\\right] = 0$;\n",
    "- дисперсія випадкових помилок однакова і кінцева, це властивість називається [гомоскедастичністю](https://uk.wikipedia.org/wiki/%D0%93%D0%BE%D0%BC%D0%BE%D1%81%D0%BA%D0%B5%D0%B4%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%BD%D1%96%D1%81%D1%82%D1%8C): $\\forall i: \\text{Var}\\left(\\epsilon_i\\right) = \\sigma^2 < \\infty$;\n",
    "- випадкові помилки не скорельовані: $\\forall i \\neq j: \\text{Cov}\\left(\\epsilon_i, \\epsilon_j\\right) = 0$.\n",
    "\n",
    "Оцінка $\\widehat{w}_i$ ваг $w_i$ називається *лінійною*, якщо\n",
    "\n",
    "$$\\large \\widehat{w}_i = \\omega_{1i}y_1 + \\omega_{2i}y_2 + \\cdots + \\omega_{1n}y_n,$$\n",
    "\n",
    "де $\\forall\\ k\\ \\omega_{ki}$ залежить лише від спостережуваних даних $\\textbf{X}$ і майже напевно нелінійно.\n",
    "\n",
    "Оскільки розв'язком завдання пошуку оптимальних ваг буде саме лінійна оцінка, то і модель називається *лінійною регресією*.\n",
    "\n",
    "Введемо ще одне визначення. Оцінка $\\widehat{w}_i$ називається *незміщенною* тоді, коли мат. очікуванння оцінки дорівнює реальному, але невідомому значенню оцінюваного параметра:\n",
    "\n",
    "$$\\large \\mathbb{E}\\left[\\widehat{w}_i\\right] = w_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один із способів обчислити значення параметрів моделі є **метод найменших квадратів** (МНК), який мінімізує середньоквадратичну помилку між реальним значенням залежної змінної і прогнозом, що видане моделлю:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\\mathcal{L}\\left(\\textbf{X}, \\textbf{y}, \\textbf{w} \\right) &=& \\frac{1}{2n} \\sum_{i=1}^n \\left(y_i - \\textbf{w}^{\\text{T}} \\textbf{x}_i\\right)^2 \\\\\n",
    "&=& \\frac{1}{2n} \\left\\| \\textbf{y} - \\textbf{X} \\textbf{w} \\right\\|_2^2 \\\\\n",
    "&=& \\frac{1}{2n} \\left(\\textbf{y} - \\textbf{X} \\textbf{w}\\right)^{\\text{T}} \\left(\\textbf{y} - \\textbf{X} \\textbf{w}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Для вирішення даної оптимізаційної задачі необхідно обчислити похідні за параметрами моделі, прирівняти їх до нуля і розв'язати отримані рівняння щодо $\\textbf w$ (матричне диференціювання непідготовленому студентові може здатися складним, тому для кращого розуміння спробуйте розписати все через суми, щоби переконатися у відповіді):\n",
    "\n",
    "Шпаргалка за матричними похідними:\n",
    "\n",
    "\n",
    "$$\\large \\begin{array}{rcl} \n",
    "\\frac{\\partial}{\\partial \\textbf{X}} \\textbf{X}^{\\text{T}} \\textbf{A} &=& \\textbf{A} \\\\\n",
    "\\frac{\\partial}{\\partial \\textbf{X}} \\textbf{X}^{\\text{T}} \\textbf{A} \\textbf{X} &=& \\left(\\textbf{A} + \\textbf{A}^{\\text{T}}\\right)\\textbf{X} \\\\\n",
    "\\frac{\\partial}{\\partial \\textbf{A}} \\textbf{X}^{\\text{T}} \\textbf{A} \\textbf{y} &=&  \\textbf{X}^{\\text{T}} \\textbf{y}\\\\\n",
    "\\frac{\\partial}{\\partial \\textbf{X}} \\textbf{A}^{-1} &=& -\\textbf{A}^{-1} \\frac{\\partial \\textbf{A}}{\\partial \\textbf{X}} \\textbf{A}^{-1} \n",
    "\\end{array}$$\n",
    "\n",
    "Продовжимо:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \\frac{\\partial \\mathcal{L}}{\\partial \\textbf{w}} &=& \\frac{\\partial}{\\partial \\textbf{w}} \\frac{1}{2n} \\left( \\textbf{y}^{\\text{T}} \\textbf{y} -2\\textbf{y}^{\\text{T}} \\textbf{X} \\textbf{w} + \\textbf{w}^{\\text{T}} \\textbf{X}^{\\text{T}} \\textbf{X} \\textbf{w}\\right) \\\\\n",
    "&=& \\frac{1}{2n} \\left(-2 \\textbf{X}^{\\text{T}} \\textbf{y} + 2\\textbf{X}^{\\text{T}} \\textbf{X} \\textbf{w}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \\frac{\\partial \\mathcal{L}}{\\partial \\textbf{w}} = 0 &\\Leftrightarrow& \\frac{1}{2n} \\left(-2 \\textbf{X}^{\\text{T}} \\textbf{y} + 2\\textbf{X}^{\\text{T}} \\textbf{X} \\textbf{w}\\right) = 0 \\\\\n",
    "&\\Leftrightarrow& -\\textbf{X}^{\\text{T}} \\textbf{y} + \\textbf{X}^{\\text{T}} \\textbf{X} \\textbf{w} = 0 \\\\\n",
    "&\\Leftrightarrow& \\textbf{X}^{\\text{T}} \\textbf{X} \\textbf{w} = \\textbf{X}^{\\text{T}} \\textbf{y} \\\\\n",
    "&\\Leftrightarrow& \\textbf{w} = \\left(\\textbf{X}^{\\text{T}} \\textbf{X}\\right)^{-1} \\textbf{X}^{\\text{T}} \\textbf{y}\n",
    "\\end{array}$$\n",
    "\n",
    "Отже, маючи на увазі всі визначення та умови, що описані вище, можемо стверджувати, опираючись на [теорему Маркова-Гаусса](https://uk.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%93%D0%B0%D1%83%D1%81%D1%81%D0%B0_%E2%80%94_%D0%9C%D0%B0%D1%80%D0%BA%D0%BE%D0%B2%D0%B0), що оцінка МНК є найкращою оцінкою параметрів моделі, поміж усіх *лінійних* і *незміщених* оцінок, тобто має найменшу дисперсію."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.1.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue; font-size:1em;\">4.1.2. Метод максимальної правдоподібності</span>\n",
    "\n",
    "[Повернутися до змісту](#4-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У студентів цілком резонно могли виникнути запитання: наприклад, чому ми мінімізуємо середньоквадратичну помилку, а не щось інше. Адже можна мінімізувати середнє абсолютне значення нев'язки або ще щось. Єдине, що відбудеться в разі зміни значення, що мінімізується, так це те, що ми вийдемо з умов теореми Маркова-Гауса й наші оцінки перестануть бути кращими поміж лінійних і незміщених.\n",
    "\n",
    "Давайте перед тим як продовжити, зробимо ліричний відступ, щоб проілюструвати метод максимальної правдоподібності на простому прикладі.\n",
    "\n",
    "Чомусь випускники вітчизняних шкіл часто пам'ятають формулу етилового спирту. Давайте проведемо уявний експеримент: чи пам'ятають люди більш просту формулу метилового спирту: $CH_3OH$. Припустимо ми опитали 400 осіб і виявилося, що формулу пам'ятають всього 117 осіб. Розумно припустити, що ймовірність того, що наступний опитаний знає формулу метилового спирту – $\\frac{117}{400} \\approx 29\\%$. Покажемо, що така інтуїтивно зрозуміла оцінка не просто хороша, а ще й є оцінкою максимальної правдоподібності.\n",
    "\n",
    "Розберемося, звідки береться ця оцінка, а для цього згадаємо визначення [розподілу Бернуллі](https://uk.wikipedia.org/wiki/%D0%A0%D0%BE%D0%B7%D0%BF%D0%BE%D0%B4%D1%96%D0%BB_%D0%91%D0%B5%D1%80%D0%BD%D1%83%D0%BB%D0%BB%D1%96): випадкова величина $X$ має розподіл Бернуллі, якщо вона приймає лише два значення ($1$ і $0$ з вірогідністю $\\theta$ і $1 - \\theta$ відповідно) і має таку функцію розподілу ймовірності:\n",
    "\n",
    "$$\\large p\\left(\\theta, x\\right) = \\theta^x \\left(1 - \\theta\\right)^\\left(1 - x\\right),~x \\in \\left\\{0, 1\\right\\}$$\n",
    "\n",
    "Здається цей розподіл – саме те, що нам потрібно, а параметр розподілу $\\theta$ і є тією оцінкою ймовірності того, що людина знає формулу метилового спирту. Ми виконали $400$ *незалежних* експериментів; позначимо їхні результати як $\\textbf{x} = \\left(x_1, x_2, \\ldots, x_{400}\\right)$. Запишемо *правдоподібність* наших даних (спостережень), тобто ймовірність спостерігати 117 реалізацій випадкової величини $X = 1$ і 283 реалізації $X = 0$:\n",
    "\n",
    "$$\\large p(\\textbf{x}; \\theta) = \\prod_{i=1}^{400} \\theta^{x_i} \\left(1 - \\theta\\right)^{\\left(1 - x_i\\right)} = \\theta^{117} \\left(1 - \\theta\\right)^{283}$$\n",
    "\n",
    "Далі будемо максимізувати цей вираз за $\\theta$, і найчастіше це роблять не з правдоподібністю $p(\\textbf{x}; \\theta)$, а з його логарифмом (застосування монотонного перетворення не змінить рішення, але спростить обчислення):\n",
    "\n",
    "$$\\large \\log p(\\textbf{x}; \\theta) = \\log \\prod_{i=1}^{400} \\theta^{x_i} \\left(1 - \\theta\\right)^{\\left(1 - x_i\\right)} = $$\n",
    "$$ \\large = \\log \\theta^{117} \\left(1 - \\theta\\right)^{283} =  117 \\log \\theta + 283 \\log \\left(1 - \\theta\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер ми хочемо знайти таке значення $\\theta$, яке максимізує правдоподібність, для цього ми візьмемо похідну за $\\theta$, прирівняємо до нуля і розв'яжемо отримане рівняння:\n",
    "\n",
    "$$\\large  \\frac{\\partial p(\\textbf{x}; \\theta)}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left(117 \\log \\theta + 283 \\log \\left(1 - \\theta\\right)\\right) = \\frac{117}{\\theta} - \\frac{283}{1 - \\theta};$$\n",
    "\n",
    "\n",
    "$$\\large \\begin{array}{rcl} \n",
    "\\frac{117}{\\theta} - \\frac{283}{1 - \\theta} = 0 \\Rightarrow \\theta = \\frac{117}{400}\n",
    "\\end{array}.$$\n",
    "\n",
    "Виходить, що наша інтуїтивна оцінка - це і є оцінка максимальної правдоподібності. Застосуємо тепер ті ж міркування для задачі лінійної регресії та спробуємо з'ясувати, що лежить за середньоквадратичною помилкою. Для цього нам доведеться подивитися на лінійну регресію з ймовірнісної точки зору. Модель, природно, залишається такою ж:\n",
    "\n",
    "$$\\large \\textbf y = \\textbf X \\textbf w + \\epsilon,$$\n",
    "\n",
    "але будемо тепер вважати, що випадкові помилки беруться з центрированного [нормального розподілу](https://uk.wikipedia.org/wiki/%D0%9D%D0%BE%D1%80%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D0%B8%D0%B9_%D1%80%D0%BE%D0%B7%D0%BF%D0%BE%D0%B4%D1%96%D0%BB):\n",
    "\n",
    "$$\\large \\epsilon_i \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)$$\n",
    "\n",
    "Перепишемо модель в новому світлі:\n",
    "\n",
    "$$\\large \\begin{array}{rcl} \n",
    "y_i &=& \\sum_{j=0}^m w_j X_{ij} + \\epsilon_i \\\\\n",
    "&\\sim& \\sum_{j=0}^m w_j  X_{ij} + \\mathcal{N}\\left(0, \\sigma^2\\right) \\\\\n",
    "p\\left(\\textbf{y} \\mid \\textbf X; \\textbf{w}\\right) &=& \\mathcal{N}\\left(\\sum_{j=0}^m w_j X_{ij}, \\sigma^2\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Оскільки приклади беруться незалежно (помилки не скорельовані – одна з умов теореми Маркова-Гаусса), то повна правдоподібність даних матиме вигляд як добуток функцій щільності $p\\left(y_i\\right)$. Розглянемо логарифм правдоподібності, що дасть нам змогу перейти від добутку до суми:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "\\log p\\left(\\textbf{y} \\mid \\textbf X; \\textbf{w}\\right) &=& \\log \\prod_{i=1}^n \\mathcal{N}\\left(\\sum_{j=0}^m w_j X_{ij}, \\sigma^2\\right) \\\\\n",
    "&=& \\sum_{i=1}^n \\log \\mathcal{N}\\left(\\sum_{j=0}^m w_j X_{ij}, \\sigma^2\\right) \\\\\n",
    "&=& -\\frac{n}{2}\\log 2\\pi\\sigma^2 -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\left(y_i - \\textbf{w}^{\\text{T}} \\textbf{x}_i\\right)^2\n",
    "\\end{array}$$\n",
    "\n",
    "Ми хочемо знайти гіпотезу максимальної правдоподібності, тобто нам потрібно максимізувати вираз $p\\left(\\textbf{y} \\mid \\textbf X; \\textbf{w}\\right)$ за вектором $\\textbf w$, отримавши водночас $\\textbf{w}_{\\text{ML}}$. Зверніть увагу, що під час максимізації функції за якомось параметром можна викинути всі члени, які не залежать від цього параметра:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "\\textbf{w}_{\\text{ML}} &=& \\arg \\max_{\\textbf w} p\\left(\\textbf{y}\\mid \\textbf X; \\textbf{w}\\right) = \\arg \\max_{\\textbf w} \\log p\\left(\\textbf{y}\\mid \\textbf X; \\textbf{w}\\right)\\\\\n",
    "&=& \\arg \\max_{\\textbf w} -\\frac{n}{2}\\log 2\\pi\\sigma^2 -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\left(y_i - \\textbf{w}^{\\text{T}} \\textbf{x}_i\\right)^2 \\\\\n",
    "&=& \\arg \\max_{\\textbf w} -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\left(y_i - \\textbf{w}^{\\text{T}} \\textbf{x}_i\\right)^2 \\\\\n",
    "&=&  \\arg \\max_{\\textbf w} \\mathcal{L}\\left(\\textbf X, \\textbf{y}, \\textbf{w} \\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Отже, ми побачили, що максимізація правдоподібності даних – це те ж саме, що і мінімізація середньоквадратичної помилки (за справедливості зазначених вище припущень). Виходить, що саме така функція вартості є наслідком того, що помилка розподілена нормально, а не якось по-іншому."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.1.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue; font-size:1em;\">4.1.3. Розкладання помилки на зміщення і розкид (Bias-variance decomposition)</span>\n",
    "\n",
    "[Повернутися до змісту](#4-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут поговоримо трохи про властивості оцінки, що отримана лінійною регресією. З попереднього пункту ми з'ясували, що\n",
    "- істинне значення цільової змінної складається з деякої детермінованої функції $f(\\textbf{x})$ та випадкової помилки $\\epsilon$: $y = f\\left(\\textbf{x}\\right) + \\epsilon$;\n",
    "- помилка розподілена нормально з центром в нулі і деяким розкидом: $\\epsilon \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)$;\n",
    "- істинне значення цільової змінної теж розподілено нормально: $y \\sim \\mathcal{N}\\left(f\\left(\\textbf{x}\\right), \\sigma^2\\right)$\n",
    "- ми намагаємося наблизити детерміновану, але невідому функцію $f\\left(\\textbf{x}\\right)$ до лінійної функції від регресорів $\\widehat{f}\\left(\\textbf{x}\\right)$, яка, зокрема, є точковою оцінкою функції $f$ в просторі функцій (точніше, ми обмежили простір функцій параметричним сімейством лінійних функцій), тобто випадкової змінної, у якій є середнє значення і дисперсія.\n",
    "\n",
    "Тоді помилка в точці $\\textbf{x}$ розкладається в такий спосіб:\n",
    "\n",
    "$$\\large \\begin{array}{rcl} \n",
    "\\text{Err}\\left(\\textbf{x}\\right) &=& \\mathbb{E}\\left[\\left(y - \\widehat{f}\\left(\\textbf{x}\\right)\\right)^2\\right] \\\\\n",
    "&=& \\mathbb{E}\\left[y^2\\right] + \\mathbb{E}\\left[\\left(\\widehat{f}\\left(\\textbf{x}\\right)\\right)^2\\right] - 2\\mathbb{E}\\left[y\\widehat{f}\\left(\\textbf{x}\\right)\\right] \\\\\n",
    "&=& \\mathbb{E}\\left[y^2\\right] + \\mathbb{E}\\left[\\widehat{f}^2\\right] - 2\\mathbb{E}\\left[y\\widehat{f}\\right] \\\\\n",
    "\\end{array}$$\n",
    "\n",
    "Для наочності опустимо позначення аргументу функцій. Розглянемо кожен член окремо, перші два розписуються легко за формулою $\\text{Var}\\left(z\\right) = \\mathbb{E}\\left[z^2\\right] - \\mathbb{E}\\left[z\\right]^2$:\n",
    "\n",
    "$$\\large \\begin{array}{rcl} \n",
    "\\mathbb{E}\\left[y^2\\right] &=& \\text{Var}\\left(y\\right) + \\mathbb{E}\\left[y\\right]^2 = \\sigma^2 + f^2\\\\\n",
    "\\mathbb{E}\\left[\\widehat{f}^2\\right] &=& \\text{Var}\\left(\\widehat{f}\\right) + \\mathbb{E}\\left[\\widehat{f}\\right]^2 \\\\\n",
    "\\end{array}$$\n",
    "\n",
    "Пояснення:\n",
    "\n",
    "$$\\large \\begin{array}{rcl} \n",
    "\\text{Var}\\left(y\\right) &=& \\mathbb{E}\\left[\\left(y - \\mathbb{E}\\left[y\\right]\\right)^2\\right] \\\\\n",
    "&=& \\mathbb{E}\\left[\\left(y - f\\right)^2\\right] \\\\\n",
    "&=& \\mathbb{E}\\left[\\left(f + \\epsilon - f\\right)^2\\right] \\\\\n",
    "&=& \\mathbb{E}\\left[\\epsilon^2\\right] = \\sigma^2\n",
    "\\end{array}$$\n",
    "\n",
    "$$\\large \\mathbb{E}[y] = \\mathbb{E}[f + \\epsilon] = \\mathbb{E}[f] + \\mathbb{E}[\\epsilon] = f$$\n",
    "\n",
    "І тепер останній член суми. Ми пам'ятаємо, що помилка і цільова змінна незалежні один від одного:\n",
    "\n",
    "$$\\large \\begin{array}{rcl} \n",
    "\\mathbb{E}\\left[y\\widehat{f}\\right] &=& \\mathbb{E}\\left[\\left(f + \\epsilon\\right)\\widehat{f}\\right] \\\\\n",
    "&=& \\mathbb{E}\\left[f\\widehat{f}\\right] + \\mathbb{E}\\left[\\epsilon\\widehat{f}\\right] \\\\\n",
    "&=& f\\mathbb{E}\\left[\\widehat{f}\\right] + \\mathbb{E}\\left[\\epsilon\\right] \\mathbb{E}\\left[\\widehat{f}\\right]  = f\\mathbb{E}\\left[\\widehat{f}\\right]\n",
    "\\end{array}$$\n",
    "\n",
    "Нарешті, збираємо все разом:\n",
    "\n",
    "$$\\large \\begin{array}{rcl} \n",
    "\\text{Err}\\left(\\textbf{x}\\right) &=& \\mathbb{E}\\left[\\left(y - \\widehat{f}\\left(\\textbf{x}\\right)\\right)^2\\right] \\\\\n",
    "&=& \\sigma^2 + f^2 + \\text{Var}\\left(\\widehat{f}\\right) + \\mathbb{E}\\left[\\widehat{f}\\right]^2 - 2f\\mathbb{E}\\left[\\widehat{f}\\right] \\\\\n",
    "&=& \\left(f - \\mathbb{E}\\left[\\widehat{f}\\right]\\right)^2 + \\text{Var}\\left(\\widehat{f}\\right) + \\sigma^2 \\\\\n",
    "&=& \\text{Bias}\\left(\\widehat{f}\\right)^2 + \\text{Var}\\left(\\widehat{f}\\right) + \\sigma^2\n",
    "\\end{array}$$\n",
    "\n",
    "Итак, мы достигли цели всех вычислений, описанных выше, последняя формула говорит нам, что ошибка прогноза любой модели вида $y = f\\left(\\textbf{x}\\right) + \\epsilon$ складывается из:\n",
    "\n",
    "Врешті ми завершили обчислення, що описані вище. Остання формула показує, що помилка прогнозу будь-якої моделі виду $y = f\\left(\\textbf{x}\\right) + \\epsilon$ складається з:\n",
    "\n",
    "- квадрата зміщення: $\\text{Bias}\\left(\\widehat{f}\\right)$ – середня помилка за усіма можливими наборами даних;\n",
    "- дисперсії: $\\text{Var}\\left(\\widehat{f}\\right)$ – варіативність помилки, то, на скільки помилка буде відрізнятися, якщо навчати модель за різними наборами даних;\n",
    "- неусовної помилки: $\\sigma^2$.\n",
    "\n",
    "Якщо з останньою частиною ми нічого зробити не можемо, то на перші два доданки ми можемо повпливати. В ідеалі звісно хотілося б звести нанівець обидва цих доданки (лівий верхній квадрат малюнка), але на практиці часто доводиться балансувати між зміщеними і нестабільними оцінками (висока дисперсія)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://raw.githubusercontent.com/radiukpavlo/Intelligent-data-analysis/main/03_img/4_1_1_bvtf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Як правило, у разі збільшення складності моделі (наприклад, під час збільшення кількості вільних параметрів) зменшується дисперсія (розкид) оцінки, але оцінка стає зміщеною. Через те, що навчальний набір даних повністю запам'ятовується замість узагальнення, невеликі зміни призводять до несподіваних результатів (наприклад, перенавчання). Якщо ж модель слабка, то вона не в змозі виявити закономірність; в результаті вивчається щось інше, зміщене щодо правильного рішення."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://raw.githubusercontent.com/radiukpavlo/Intelligent-data-analysis/main/03_img/4_1_2_biasvariance.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теорема Маркова-Гаусса якраз стверджує, що МНК-оцінка параметрів лінійної моделі є найкращою в класі незміщених лінійних оцінок, тобто з найменшою дисперсією. Це означає, що у разі існування будь-якої іншої незміщенної моделі $g$ з класу лінійних моделей, то ми можемо бути впевнені, що\n",
    "$Var\\left(\\widehat{f}\\right) \\leq Var\\left(g\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.1.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue; font-size:1em;\">4.1.4. Регуляризація лінійної регресії</span>\n",
    "\n",
    "[Повернутися до змісту](#4-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Іноді трапляються ситуації, за яких ми навмисно збільшуємо зміщеність моделі для забезпечення її стабільності, тобто для зменшення дисперсії моделі $\\text{Var}\\left(\\widehat{f}\\right)$. Однією з умов теореми Маркова-Гаусса є повний стовпцевий ранг матриці $\\textbf{X}$. В іншому випадку рішення МНК $\\textbf{w} = \\left(\\textbf{X}^{\\text{T}} \\textbf{X}\\right)^{-1} \\textbf{X}^{\\text{T}} \\textbf{y}$ не існує, тому що не існуватиме зворотна матриця $\\left(\\textbf{X}^{\\text{T}} \\textbf{X}\\right)^{-1}$. Іншими словами, матриця $\\textbf{X}^{\\text{T}} \\textbf{X}$ буде [виродженою](https://uk.wikipedia.org/wiki/%D0%92%D0%B8%D1%80%D0%BE%D0%B4%D0%B6%D0%B5%D0%BD%D0%B0_%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86%D1%8F). Така задача називається [некоректно поставленою](https://uk.wikipedia.org/wiki/%D0%9A%D0%BE%D1%80%D0%B5%D0%BA%D1%82%D0%BD%D0%BE_%D0%BF%D0%BE%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B0_%D0%B7%D0%B0%D0%B4%D0%B0%D1%87%D0%B0). Задачу потрібно скорегувати, а саме, зробити матрицю $\\textbf{X}^{\\text{T}}\\textbf{X}$ невиродженою, або регульованою (саме тому цей процес називається регуляризацією). Найчастіше в даних ми можемо спостерігати так звану [мультиколінеарність](https://uk.wikipedia.org/wiki/%D0%9C%D1%83%D0%BB%D1%8C%D1%82%D0%B8%D0%BA%D0%BE%D0%BB%D1%96%D0%BD%D0%B5%D0%B0%D1%80%D0%BD%D1%96%D1%81%D1%82%D1%8C) — коли два або кілька ознак сильно корельовані, в матриці $\\textbf{X}$ це проявляється у вигляді \"майже\" лінійної залежності стовпців. Наприклад, в задачі прогнозування ціни квартири за її параметрами \"майже\" лінійна залежність буде в ознак \"площа з урахуванням балкона\" і \"площа без урахування балкона\". Формально для таких даних матриця $\\textbf{X}^{\\text{T}} \\textbf{X}$ буде оберненою, але через мультиколінеарність у матриці $\\textbf{X}^{\\text{T}} \\textbf{X}$ деякі власні значення будуть близькі до нуля, а в оберненій матриці $\\left(\\textbf{X}^{\\text{T}} \\textbf{X}\\right)^{-1}$ з'являться екстремально великі власні значення, тому що власні значення оберненої матриці – це $\\frac{1}{\\lambda_i}$. Підсумком такого хитання власних значень стане нестабільна оцінка параметрів моделі, тобто додавання нового спостереження в набір навчальних даних призведе до зовсім іншого результату.\n",
    "\n",
    "Ілюстрації зростання коефіцієнтів можна переглянути в [туторіалі](https://habr.com/ru/company/ods/blog/322076/). Одним із способів регуляризації є [регуляризація Тихонова](https://uk.wikipedia.org/wiki/%D0%A0%D0%B5%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D1%96%D1%8F_(%D0%BC%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D0%BA%D0%B0)#%D0%A0%D0%B5%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D1%96%D1%8F_%D0%A2%D0%B8%D1%85%D0%BE%D0%BD%D0%BE%D0%B2%D0%B0) (a.k.a. L2-регуляризація), яка в загальному вигляді виглядає як додавання нового члена до середньоквадратичної помилки: \n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "\\mathcal{L}\\left(\\textbf{X}, \\textbf{y}, \\textbf{w} \\right) &=& \\frac{1}{2n} \\left\\| \\textbf{y} - \\textbf{X} \\textbf{w} \\right\\|_2^2 + \\left\\|\\Gamma \\textbf{w}\\right\\|^2\\\\\n",
    "\\end{array}$$\n",
    "\n",
    "Зачасту матриця Тихонова виражається як добуток деякого числа на одиничну матрицю: $\\Gamma = \\frac{\\lambda}{2}  \\textbf{E}$. У цьому випадку задача мінімізації середньоквадратичної помилки стає задачею з обмеженням на $L_2$ норму. Якщо продиференціювати нову функцію вартості за параметрами моделі, прирівняти отриману функцію до нуля і виразити $\\textbf{w}$, то ми отримаємо точний розв'язок задачі.\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "\\textbf{w} &=& \\left(\\textbf{X}^{\\text{T}} \\textbf{X} + \\lambda \\textbf{E}\\right)^{-1} \\textbf{X}^{\\text{T}} \\textbf{y}\n",
    "\\end{array}$$\n",
    "\n",
    "Така регресія має назву гребнева регресія (з англ. [ridge regression](https://en.wikipedia.org/wiki/Ridge_regression)). А гребенем є якраз діагональна матриця, яку ми додаємо до матриці $\\textbf{X}^{\\text{T}} \\textbf{X}$, в результаті виходить гарантовано регулярна матриця."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://raw.githubusercontent.com/radiukpavlo/Intelligent-data-analysis/main/03_img/4_1_3_ridge.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таке рішення зменшує дисперсію, але стає зміщеним, тому що мінімізується також і норма вектора параметрів, що змушує результат зміщуватися в бік нуля. На рисунку нижче на перетині білих пунктирних ліній знаходиться МНК-рішення. Блакитними точками позначені різні рішення гребеневої регресії. Бачимо, що за збільшення параметра регуляризації $\\lambda$ кінцевий результат зміщується в бік нуля."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://raw.githubusercontent.com/radiukpavlo/Intelligent-data-analysis/main/03_img/4_1_4_l2-regularization.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Насамкінець, студентам пропонується звернутися до посту [Open Data Science](https://habrahabr.ru/company/ods/blog/322076/) на Хабрахабр за прикладом того, як $L_2$-регулярізація справляється з проблемою мультиколінеарності, а також щоб освіжити в пам'яті ще кілька інтерпретацій регуляризації."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">4.2. Логістична регресія та метод максимальної правдоподібності</span>\n",
    "\n",
    "[Повернутися до змісту](#4-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.2.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue; font-size:1em;\">4.2.1. Лінійний класифікатор</span>\n",
    "\n",
    "[Повернутися до змісту](#4-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основна ідея лінійного класифікатора полягає в тому, що простір ознак може бути розділений гіперплощиною на дві півплощини, в кожній з яких прогнозується одне з двох значень цільового класу:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://raw.githubusercontent.com/radiukpavlo/Intelligent-data-analysis/main/03_img/4_1_5_logit.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Якщо це можна зробити без помилок, то навчальна вибірка називається *лінійно роздільною*.\n",
    "\n",
    "Ми вже знайомі з лінійною регресією і методом найменших квадратів. Розглянемо тепер задачу бінарної класифікації, причому мітки цільового класу позначимо \"+1\" (позитивні приклади) і \"-1\" (негативні приклади).\n",
    "Один з найпростіших лінійних класифікаторів на основі регресії має такий вигляд:\n",
    "\n",
    "$$\\Large a(\\textbf{x}) = \\text{sign}(\\textbf{w}^{\\text{T}}\\textbf x),$$\n",
    "\n",
    "де\n",
    " - $\\textbf{x}$ – вектор ознак прикладу (разом з одиницею);\n",
    " - $\\textbf{w}$ – ваги лінійної моделі (разом зі зміщенням $w_0$);\n",
    " - $\\text{sign}(\\bullet)$ – функція \"сігнум\", що повертає знак свого аргументу;\n",
    " - $a(\\textbf{x})$ – відповідь класифікатора за прикладом $\\textbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.2.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue; font-size:1em;\">4.2.2. Логістична регресія, як лінійний класифікатор</span>\n",
    "\n",
    "[Повернутися до змісту](#4-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логістична регресія є окремим випадком лінійного класифікатора, але вона має гарне \"вміння\" – прогнозувати ймовірність $p_+$ віднесення прикладу $\\textbf{x}_\\text{i}$ до класу \"+\":\n",
    "\n",
    "$$\\Large p_+ = \\text P\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) $$\n",
    "\n",
    "Прогнозування не просто відповіді (\"+1\" або \"-1\"), а саме *ймовірності* віднесення до класу \"+1\" у багатьох задачах є надзвичайно важливою бізнес-вимогою. Наприклад, в задачі кредитного скорингу, де традиційно застосовується логістична регресія, часто прогнозують ймовірність неповернення кредиту ($p_+$). Клієнтів, які звернулися за кредитом, сортують за цією передбачуваною ймовірністю (за спадною), і отримують оціночну карту — де факто, рейтинг клієнтів від поганих до хороших. Нижче подамо іграшковий приклад такої оціночної карти."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://raw.githubusercontent.com/radiukpavlo/Intelligent-data-analysis/main/03_img/4_1_6_toy_scorecard_eng.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Банк вибирає для себе поріг $p_*$ передбаченої ймовірності неповернення кредиту (на рисунку – $0.15$) і починаючи з цього значення вже не видає кредит. Більш того, можна помножити передбачувану ймовірність на видану суму і отримати мат. очікування втрат від клієнта, що теж буде хорошою бізнес-метрикою."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отже, ми хочемо прогнозувати ймовірність $p_+ \\in [0,1]$, а поки вміємо будувати лінійний прогноз за допомогою МНК:\n",
    "\n",
    "$$\\Large b(\\textbf{x}) = \\textbf{w}^\\text{T} \\textbf{x} \\in \\mathbb{R}.$$\n",
    "\n",
    "Яким чином перетворити отримане значення в ймовірність, межі якої – [0; 1]? Очевидно, для цього потрібна деяка функція $f: \\mathbb{R} \\rightarrow [0,1]$. У моделі логістичної регресії для цього береться конкретна функція: $\\sigma(z) = \\frac{1}{1 + \\exp^{-z}}$.\n",
    "\n",
    "Далі попрацюємо з кодом, щоби розібратися, які для цього є передумови."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# відключимо всякі попередження Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(z):\n",
    "    return 1. / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = np.linspace(-10, 10, 1000)\n",
    "plt.plot(xx, [sigma(x) for x in xx]);\n",
    "plt.xlabel('z');\n",
    "plt.ylabel('sigmoid(z)')\n",
    "plt.title('Sigmoid function');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Позначимо через $P(X)$ ймовірність події $X$. Тоді відношення ймовірностей $OR(X)$ визначається з $\\frac{P(X)}{1-P(X)}$, а це — відношення ймовірностей того, відбудеться подія або ні. Очевидно, що ймовірність і відношення шансів містять однакову інформацію. Але в той час як $P(X)$ перебуває в межах від 0 до 1, $OR(X)$ є в межах від 0 до $\\infty$.\n",
    "\n",
    "Якщо обчислити логарифм $OR(X)$ (логарифм шансів, або логарифм відношення ймовірностей), то легко помітити, що $\\log{OR(X)} \\in \\mathbb{R}$. Його то ми і будемо прогнозувати за допомогою МНК\n",
    "\n",
    "Поглянемо, як логістична регресія робитиме прогноз $p_+ = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right)$ (поки вважаємо, що ваги $\\textbf{w}$ ми якось отримали (тобто навчили модель), далі розберемося, як саме).\n",
    "\n",
    "**Крок 1.** Обчислити значення $w_{0}+w_{1}x_1 + w_{2}x_2 + ... = \\textbf{w}^\\text{T}\\textbf{x}$. (рівняння $\\textbf{w}^\\text{T}\\textbf{x} = 0$ задає гіперплощини, що розділяє приклади на 2 класи);\n",
    "\n",
    "\n",
    "**Крок 2.** Обчислити логарифм відношення шансів: $ \\log(OR_{+}) =  \\textbf{w}^\\text{T}\\textbf{x}$.\n",
    "\n",
    "**Крок 3.** Маючи прогноз шансів на віднесення до класу \"+\" – $OR_{+}$, обчислити $p_{+}$ за допомогою простої залежності:\n",
    "\n",
    "$$\\Large p_{+} = \\frac{OR_{+}}{1 + OR_{+}} = \\frac{\\exp^{\\textbf{w}^\\text{T}\\textbf{x}}}{1 + \\exp^{\\textbf{w}^\\text{T}\\textbf{x}}} =  \\frac{1}{1 + \\exp^{-\\textbf{w}^\\text{T}\\textbf{x}}} = \\sigma(\\textbf{w}^\\text{T}\\textbf{x})$$\n",
    "\n",
    "\n",
    "У правій частині ми отримали якраз сигмоїд-функцію.\n",
    "\n",
    "Отже, логістична регресія прогнозує ймовірність віднесення прикладу до класу \"+\" (за умови, що ми знаємо його ознаки і ваги моделі) як сигмоїд-перетворення лінійної комбінації вектора ваг моделі і вектора ознак прикладу:\n",
    "\n",
    "$$\\Large p_+(x_i) = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}). $$\n",
    "\n",
    "Наступне питання: як модель навчається. Тут ми знову звертаємося до принципу максимальної правдоподібності."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.2.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue; font-size:1em;\">4.2.3. Принцип максимальної правдоподібності та логістична регресія</span>\n",
    "\n",
    "[Повернутися до змісту](#4-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер подивимося, як з принципу максимальної правдоподібності виходить оптимізаційна задача, яку вирішує логістична регресія, а саме, – мінімізація *логістичної* функції втрат.\n",
    "Вище ми побачили, що логістична регресія моделює ймовірність віднесення прикладу до класу \"+\" як\n",
    "\n",
    "$$\\Large p_+(\\textbf{x}_\\text{i}) = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i})$$\n",
    "\n",
    "Тоді для класу \"-\" аналогічна ймовірність:\n",
    "$$\\Large p_-(\\textbf{x}_\\text{i})  = \\text{P}\\left(y_i = -1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right)  = 1 - \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}) = \\sigma(-\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}) $$\n",
    "\n",
    "Обидва цих вирази можна легко об'єднати в одне:\n",
    "\n",
    "$$\\Large \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i})$$\n",
    "\n",
    "Вираз $M(\\textbf{x}_\\text{i}) = y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}$ називається *відступом* (*margin*) класифікації на об'єкті $\\textbf{x}_\\text{i}$ (не плутати з зазором (теж margin), про який найчастіше говорять в контексті [методу опорних векторів](https://uk.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D0%B8%D1%85_%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D1%96%D0%B2)). Якщо він невід'ємний, то модель не помиляється на об'єкті$\\textbf{x}_\\text{i}$, якщо ж від'ємний – клас для $\\textbf{x}_\\text{i}$ спрогнозований неправильно.\n",
    "Зауважимо, що відступ визначено для об'єктів саме навчальної вибірки, для яких відомі реальні мітки цільового класу $y_i$.\n",
    "\n",
    "Щоб зрозуміти, чому ми зробили саме такі висновки, звернемося до геометричної інтерпретації лінійного класифікатора. Детально про це можна почитати в матеріалах Євгенія Соколова – [тут](https://github.com/esokolov/ml-course-msu/blob/master/ML16/lecture-notes/Sem09_linear.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для глибшого розуміння лінійної класифікації можна спробувати розв'язати майже класичну задачу з початкового курсу лінійної алгебри: знайти відстань від точки з радіус-вектором $\\textbf{x}_A$ до площини, яка задається рівнянням $\\textbf{w}^\\text{T}\\textbf{x} = 0.$\n",
    "\n",
    "Відповідь:\n",
    "\n",
    "$$\\Large \\rho(\\textbf{x}_A, \\textbf{w}^\\text{T}\\textbf{x} = 0) = \\frac{\\textbf{w}^\\text{T}\\textbf{x}_A}{||\\textbf{w}||}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://raw.githubusercontent.com/radiukpavlo/Intelligent-data-analysis/main/03_img/4_1_7_simple-linal-task.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коли отримаємо (або подивимося) відповідь, то зрозуміємо, що чим більший за модулем вираз $\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}$, тим далі точка $\\textbf{x}_\\text{i}$ знаходиться від площини $\\textbf{w}^{\\text{T}}\\textbf{x} = 0.$\n",
    "\n",
    "Тому вираз $M(\\textbf{x}_\\text{i}) = y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}$ – це свого роду \"впевненість\" моделі в класифікації об'єкта $\\textbf{x}_\\text{i}$: \n",
    "\n",
    "- якщо відступ великий (за модулем) і додатний, це означає, що мітка класу поставлена правильно, а об'єкт знаходиться далеко від розділяючої гіперплощини (такий об'єкт класифікується впевнено). На рисунку – $x_3$.\n",
    "- якщо відступ великий (за модулем) і від'ємний, то мітка класу поставлена неправильно, а об'єкт знаходиться далеко від розділяючої гіперплощини (швидше за все такий об'єкт – аномалія, наприклад, його мітка в навчальній вибірці поставлена неправильно). На рисунку – $x_1$.\n",
    "- якщо відступ малий (за модулем), то об'єкт знаходиться близько до розділяючої гіперплощини, а знак відступу визначає, чи правильно об'єкт класифікований. На рисунку – $x_2$ та $x_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://raw.githubusercontent.com/radiukpavlo/Intelligent-data-analysis/main/03_img/4_1_8_margin.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер розпишемо правдоподібність вибірки, а саме, ймовірність спостерігати даний вектор $\\textbf{y}$ у вибірці $\\textbf X$. Робимо сильне припущення: об'єкти приходять незалежно, з одного розподілу (*i.i.d.*). Тоді\n",
    "\n",
    "$$\\Large \\text{P}\\left(\\textbf{y} \\mid \\textbf X, \\textbf{w}\\right) = \\prod_{i=1}^{\\ell} \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right),$$\n",
    "\n",
    "де $\\ell$ – довжина вибірки $\\textbf X$ (кількість рядків).\n",
    "\n",
    "Як положено, візьмемо логарифм цього виразу (суму оптимізувати значно простіше, ніж добуток):\n",
    "\n",
    "$$\\Large  \\log \\text{P}\\left(\\textbf{y} \\mid \\textbf X, \\textbf{w}\\right) = \\log \\sum_{i=1}^{\\ell} \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\log \\prod_{i=1}^{\\ell} \\sigma(y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i})   = $$\n",
    "\n",
    "$$\\Large  = \\sum_{i=1}^{\\ell} \\log \\sigma(y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}) = \\sum_{i=1}^{\\ell} \\log \\frac{1}{1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}} = - \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тобто в даному випадку принцип максимізації правдоподібності призводить до мінімізації виразу\n",
    "\n",
    "$$\\Large \\mathcal{L_{log}} (\\textbf X, \\textbf{y}, \\textbf{w}) = \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}).$$\n",
    "\n",
    "Це *логістична* функція втрат, що просумована за усіма об'єктами навчальної вибірки.\n",
    "\n",
    "Подивимося на нову функцію, як на функцію від відступу: $L(M) = \\log (1 + \\exp^{-M})$. Побудуємо її графік, а також графік 1/0 функції втрат (*zero-one loss*), яка просто штрафує модель на 1 за помилку на кожному об'єкті (відступ від'ємний): $L_{1/0}(M) = [M < 0]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://raw.githubusercontent.com/radiukpavlo/Intelligent-data-analysis/main/03_img/4_1_9_logloss_margin_eng.png)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рисунок відображає загальну ідею, що в задачі класифікації, не вміючи безпосередньо мінімізувати кількість помилок (градієнтними методами цього не зробити – похідна 1/0 функції втрат в нулі прямує в безмежність), ми мінімізуємо деяку її верхню оцінку. В даному випадку це логістична функція втрат (де логарифм двійковий, але це не принципово), і справедливо\n",
    "\n",
    "$$\\Large \\mathcal{L_{\\text{1/0}}} (\\textbf X, \\textbf{y}, \\textbf{w}) = \\sum_{i=1}^{\\ell} [M(\\textbf{x}_\\text{i}) < 0] \\leq \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}) = \\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w}), $$\n",
    "\n",
    "де $\\mathcal{L_{\\text{1/0}}} (\\textbf X, \\textbf{y}, \\textbf{w})$ – просто кількість помилок логістичної регресії з вагами $\\textbf{w}$ за вибіркою $(\\textbf X, \\textbf{y})$.\n",
    "\n",
    "Тобто зменшуючи верхню оцінку $\\mathcal{L_{\\log}}$ на кількість помилок класифікації, ми у такий спосіб сподіваємося зменшити і саму кількість помилок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4.2.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue; font-size:1em;\">4.2.4. L2-регуляризація логістичної функції втрат</span>\n",
    "\n",
    "[Повернутися до змісту](#4-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L2-регуляризація$ логістичної регресії влаштована майже так само, як і гребенева (ridge regression). Замість функціонала $\\mathcal{L_{\\log}} (X, \\textbf{y}, \\textbf{w})$ мінімізується наступний:\n",
    "\n",
    "$$\\Large J(\\textbf X, \\textbf{y}, \\textbf{w}) = \\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w}) + \\lambda |\\textbf{w}|^2$$\n",
    "\n",
    "У разі логістичної регресії прийнято введення зворотного коефіцієнта регуляризації $C = \\frac{1}{\\lambda}$. І тоді рішенням задачі буде\n",
    "\n",
    "$$\\Large \\widehat{\\textbf{w}}  = \\arg \\min_{\\textbf{w}} J(\\textbf X, \\textbf{y}, \\textbf{w}) =  \\arg \\min_{\\textbf{w}}\\ (C\\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}})+ |\\textbf{w}|^2)$$ \n",
    "\n",
    "Далі розглянемо приклад, що дає змогу інтуїтивно зрозуміти один з сенсів регуляризації. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
